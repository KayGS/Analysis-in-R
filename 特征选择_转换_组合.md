```{r}
library(caret)
library(randomForest)
library(e1071)
library(gbm)
library(FactoMineR)
```

#在特征选择后，为什么还要进行特征转换比如使用PCA,FA等方法？
#1. 减少冗余和多重共线性
#特征选择方法可能会选择一组最重要的特征，但这些特征之间仍可能存在冗余和多重共线性（高度相关的特征）。PCA（主成分分析）和FA（因子分析）等方法通过将原始特征转换为新的、不相关的特征，可以有效地减少冗余和多重共线性，提高模型的稳定性和性能。

#2. 提高模型的可解释性
#PCA和FA通过将原始特征转换为新的主成分或因子，可以帮助我们理解数据的结构和潜在的模式。这些新的特征通常是原始特征的线性组合，具有明确的解释意义。例如，PCA的主成分表示数据中最大方差的方向，而FA的因子表示数据中的潜在变量。

#3. 降低数据的维度
#即使在特征选择后，数据的维度可能仍然较高。PCA和FA等方法通过进一步降低数据的维度，可以减少计算成本，加快模型训练和预测的速度，同时避免过拟合。

#4. 提高模型的性能
#通过将特征转换为新的、不相关的特征，可以提高模型的泛化能力和预测性能。这些方法有助于减少噪声和不相关的信息，使模型能够更好地捕捉数据中的重要模式。

#示例：在特征选择后使用PCA进行特征转换
#以下是一个R代码示例，展示如何在特征选择后使用PCA进行特征转换，然后训练GBDT模型。

# 假设这是你的数据集
```{r}
# 
set.seed(42)
data <- data.frame(
  Age = rnorm(100, mean = 50, sd = 10),
  Income = rnorm(100, mean = 50000, sd = 15000),
  JobType = sample(letters[1:5], 100, replace = TRUE),
  CreditCardUsage = rnorm(100, mean = 3000, sd = 1000),
  MaritalStatus = sample(c("Single", "Married"), 100, replace = TRUE),
  EducationLevel = sample(1:3, 100, replace = TRUE),
  LoanAmount = rnorm(100, mean = 20000, sd = 5000),
  LoanDuration = rnorm(100, mean = 60, sd = 15),
  NumberOfDependents = sample(0:3, 100, replace = TRUE),
  Defaulted = sample(c(0, 1), 100, replace = TRUE)
)
```

# 转换分类变量为因子
```{r}
#data$JobType <- as.factor(data$JobType)
data$MaritalStatus <- as.factor(data$MaritalStatus)
```

# 确保响应变量是数值型
```{r}
# data$Defaulted <- as.numeric(data$Defaulted)
```

# 设置训练控制
```{r}
control <- rfeControl(functions = rfFuncs, method = "cv", number = 10)`
```

# 进行递归特征消除
```{r}
set.seed(42)
results <- rfe(data[, -ncol(data)], data$Defaulted, sizes = c(1:10), rfeControl = control)`
```

# 查看选出的最佳特征
```{r}
selected_features <- predictors(results)
print(selected_features)
```

# 选择最佳特征后的数据集
```{r}
selected_data <- data[, c(selected_features, "Defaulted")]`

str(selected_data)
```

# 对分类变量进行One-Hot编码
```{r}
dummies_model <- dummyVars(Defaulted ~ ., data = selected_data, fullRank = TRUE)
selected_data_dummy <- predict(dummies_model, newdata = selected_data)


str(selected_data_dummy)
```

# 转换为数据框
```{r}
selected_data_dummy <- as.data.frame(selected_data_dummy)
```

# 添加响应变量
```{r}
selected_data_dummy$Defaulted <- selected_data$Defaulted
```

# 检查One-Hot编码后的数据集
```{r}
str(selected_data_dummy)
```


# 对选出的特征进行PCA转换
```{r}
pca_result <- PCA(selected_data_dummy[, -ncol(selected_data_dummy)], graph = FALSE)
pca_data <- as.data.frame(pca_result$ind$coord)

loadings<-pca_result$rotation
```

# 添加响应变量
```{r}
pca_data$Defaulted <- selected_data_dummy$Defaulted
```


# 使用GBDT模型进行训练
```{r}
set.seed(42)
gbdt_model <- gbm(Defaulted ~ ., data = pca_data, distribution = "bernoulli", n.trees = 100, interaction.depth = 3, shrinkage = 0.1, cv.folds = 5)
```

# 打印GBDT模型结果
```{r}
summary(gbdt_model)
```

# 分割数据集为训练集和测试集
```{r}
set.seed(42)
trainIndex <- createDataPartition(pca_data$Defaulted, p = 0.8, list = FALSE)
trainData <- pca_data[trainIndex, ]
testData <- pca_data[-trainIndex, ]
```

# 使用GBDT模型进行训练
```{r}
set.seed(42)
gbdt_model <- gbm(Defaulted ~ ., data = trainData, distribution = "bernoulli", n.trees = 100, interaction.depth = 3, shrinkage = 0.1, cv.folds = 5)
```

# 打印GBDT模型结果
```{r}
summary(gbdt_model)
```

# 获取最佳迭代次数
```{r}
best_iter <- gbm.perf(gbdt_model, method = "cv")
```

# 预测测试数据
```{r}
predictions <- predict(gbdt_model, newdata = testData, n.trees = best_iter, type = "response")
```

# 将预测概率转换为类别标签
```{r}
predicted_labels <- ifelse(predictions >= 0.5, 1, 0)
```

# 计算混淆矩阵
```{r}
conf_matrix <- confusionMatrix(factor(predicted_labels), factor(testData$Defaulted))
print(conf_matrix)
```

# 计算AUC
```{r}
library(pROC)
roc_curve <- roc(testData$Defaulted, predictions)
auc_value <- auc(roc_curve)
print(auc_value)
```


# 假设我们有新的测试数据
```{r}
new_test_data <- testData[1:10, -ncol(testData)]
```

# 对新数据进行预测
```{r}
new_predictions <- predict(gbdt_model, newdata = new_test_data, n.trees = best_iter, type = "response")
```

# 将预测概率转换为类别标签
```{r}
new_predicted_labels <- ifelse(new_predictions >= 0.5, 1, 0)
print(new_predicted_labels)  
```

